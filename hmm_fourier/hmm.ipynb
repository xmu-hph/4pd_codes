{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "class HMM(nn.Module):\n",
    "    def __init__(self, num_states, num_observations):\n",
    "        super(HMM, self).__init__()\n",
    "        self.num_states = num_states\n",
    "        self.num_observations = num_observations\n",
    "        # Transition matrix A (state to state)\n",
    "        self.A = nn.Parameter(torch.randn(num_states, num_states))\n",
    "        # Emission matrix B (state to observation)\n",
    "        self.B = nn.Parameter(torch.randn(num_states, num_observations))\n",
    "        # Initial state distribution Ï€\n",
    "        self.pi = nn.Parameter(torch.randn(num_states))\n",
    "\n",
    "    def forward(self, observations):\n",
    "        seq_len = len(observations)\n",
    "        log_alpha = torch.zeros(seq_len, self.num_states)\n",
    "        # Initial state probabilities\n",
    "        log_alpha[0] = torch.log_softmax(self.pi, dim=0) + torch.log_softmax(self.B, dim=1)[:, observations[0]]\n",
    "        # Forward pass\n",
    "        for t in range(1, seq_len):\n",
    "            for j in range(self.num_states):\n",
    "                log_alpha[t, j] = torch.logsumexp(log_alpha[t-1] + torch.log_softmax(self.A, dim=1)[:, j], dim=0) + torch.log_softmax(self.B, dim=1)[j,observations[t]]\n",
    "        return log_alpha\n",
    "    \n",
    "    def predict(self, observations):\n",
    "        seq_len = len(observations)\n",
    "        log_delta = torch.zeros(seq_len, self.num_states)\n",
    "        psi = torch.zeros(seq_len, self.num_states, dtype=torch.long)\n",
    "        # Initial state probabilities\n",
    "        log_delta[0] = torch.log_softmax(self.pi, dim=0) + torch.log_softmax(self.B, dim=1)[:, observations[0]]\n",
    "        # Viterbi pass\n",
    "        for t in range(1, seq_len):\n",
    "            for j in range(self.num_states):\n",
    "                max_val, max_idx = torch.max(log_delta[t-1] + torch.log_softmax(self.A, dim=1)[:, j], dim=0)\n",
    "                log_delta[t, j] = max_val + torch.log_softmax(self.B, dim=1)[j, observations[t]]\n",
    "                psi[t, j] = max_idx\n",
    "        # Backtrack\n",
    "        states = torch.zeros(seq_len, dtype=torch.long)\n",
    "        states[-1] = torch.argmax(log_delta[-1])\n",
    "        for t in range(seq_len-2, -1, -1):\n",
    "            states[t] = psi[t+1, states[t+1]]\n",
    "        return states\n",
    "\n",
    "# Generate some synthetic data for demonstration\n",
    "def generate_data(num_sequences, sequence_length, num_states, num_observations):\n",
    "    A = np.random.rand(num_states, num_states)\n",
    "    A = A / A.sum(axis=1, keepdims=True)\n",
    "    B = np.random.rand(num_states, num_observations)\n",
    "    B = B / B.sum(axis=1, keepdims=True)\n",
    "    pi = np.random.rand(num_states)\n",
    "    pi = pi / pi.sum()\n",
    "    print(A,B,pi)\n",
    "    \n",
    "    sequences = []\n",
    "    states = []\n",
    "    \n",
    "    for _ in range(num_sequences):\n",
    "        seq = []\n",
    "        state_seq = []\n",
    "        state = np.random.choice(num_states, p=pi)\n",
    "        for _ in range(sequence_length):\n",
    "            obs = np.random.choice(num_observations, p=B[state])\n",
    "            seq.append(obs)\n",
    "            state_seq.append(state)\n",
    "            state = np.random.choice(num_states, p=A[state])\n",
    "        sequences.append(seq)\n",
    "        states.append(state_seq)\n",
    "    \n",
    "    return sequences, states\n",
    "\n",
    "# Parameters\n",
    "num_states = 3\n",
    "num_observations = 5\n",
    "num_sequences = 100\n",
    "sequence_length = 10\n",
    "\n",
    "# Generate synthetic data\n",
    "sequences, _ = generate_data(num_sequences, sequence_length, num_states, num_observations)\n",
    "\n",
    "# Convert sequences to tensor\n",
    "sequences = torch.tensor(sequences, dtype=torch.long)\n",
    "\n",
    "# Initialize HMM model\n",
    "model = HMM(num_states, num_observations)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for sequence in sequences:\n",
    "        optimizer.zero_grad()\n",
    "        log_alpha = model(sequence)\n",
    "        loss = -torch.logsumexp(log_alpha[-1], dim=0)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/num_sequences}')\n",
    "\n",
    "# Test the model with a new sequence\n",
    "test_sequence = torch.tensor([0, 1, 2, 3, 4, 0, 1, 2, 3, 4], dtype=torch.long)\n",
    "predicted_states = model.predict(test_sequence)\n",
    "print(\"Predicted States:\", predicted_states)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.softmax(model.A, dim=1),torch.softmax(model.B, dim=1),torch.softmax(model.pi, dim=0)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
